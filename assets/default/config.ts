/* DÃ‰BUT : ** NE PAS MODIFIER */
/* oxlint-disable */
import { createHuggingFace } from '@ai-sdk/huggingface'
import { createAnthropic } from '@ai-sdk/anthropic'
import { createCerebras } from '@ai-sdk/cerebras'
import { createCohere } from '@ai-sdk/cohere'
import { createGoogleGenerativeAI } from '@ai-sdk/google'
import { createGroq } from '@ai-sdk/groq'
import { createMistral } from '@ai-sdk/mistral'
import { createOpenAI } from '@ai-sdk/openai'
import { createTogetherAI } from '@ai-sdk/togetherai'
import type { Config } from '../../utils/_schema'

const groq = createGroq()
const cohere = createCohere()
const openai = createOpenAI()
const mistral = createMistral()
const cerebras = createCerebras()
const anthropic = createAnthropic()
const togetherai = createTogetherAI()
const huggingface = createHuggingFace()
const google = createGoogleGenerativeAI()
/* oxlint-enable */
/* FIN : ** NE PAS MODIFIER */

//
// ASTUCE : Pour vous assurer que le fichier de configuration est correctement
// paramÃ©trÃ©, lancer dans votre terminal `bun pierre:config`
//

export default {
  // Doit Ãªtre Ã  identique au nom du dossier dans `assets/`
  id: 'default',

  // Le nom de la configuration telle que vous souhaitez qu'elle soit affichÃ©e
  // dans l'interface-utilisateur de PIERRE.
  display: 'PIERRE (dÃ©faut)',

  // Si vous paramÃ©trez PIERRE de maniÃ¨re Ã  ce qu'il dispose de plusieurs
  // configurations, vous pouvez choisir de n'afficher que certaines d'entre
  // elles dans l'interface. Par exemple, si vous avez une configuration pour
  // les candidats et une autre pour les Ã©quipes, vous pouvez choisir de
  // n'afficher que la configuration pour les candidats dans l'interface de
  // PIERRE.
  //
  // Si vous ne souhaitez pas utiliser cette fonctionnalitÃ© : show: []
  //
  // Ici, lorsque vous accÃ©dez Ã  l'interface de PIERRE, vous pourrez accÃ©der aux
  // configurations `default`, `demo_client` et `demo_team`.
  show: ['default', 'demo_client', 'demo_team'],

  // Vous pouvez communiquer des donnÃ©es externes Ã  PIERRE via son URL avec le
  // paramÃ©tre de requÃªte `data`. S'il y a plusieurs donnÃ©es, sÃ©parez-les par un
  // pipe (|). Ces donnÃ©es peuvent, aprÃ¨s avoir Ã©tÃ© tranformÃ©es, Ãªtre utilisÃ©es
  // dans un prompt. Par exemple, pour que PIERRE connaisse toujours le prÃ©nom
  // du locataire et son solde locataire.
  //
  // Ex : http://localhost:3000/?data=Luc|-7.12
  //
  // Si vous ne souhaitez pas utiliser cette fonctionnalitÃ© : custom_data: {}
  //
  // La fonction ci-aprÃ¨s transforme les donnÃ©es communiquÃ©es via `data` en une
  // `string` customisable et comprÃ©hensible par un LLM.
  custom_data: {
    format: (data: string[]) => {
      return `${data[0]} a une solde locataire de ${data[1]} euros` // "Luc a un solde locataire de -7.12 euros"
    }
  },

  // Pour intÃ©grer les conversations de PIERRE dans votre SI, vous pouvez le
  // connectez Ã  vos API. Un webhook sera Ã©mis vers vos API Ã  chaque message
  // adressÃ© Ã  PIERRE ou rÃ©pondu par PIERRE. L'URL, la clef API et le format du
  // webhook sont dÃ©finis ci-dessous.
  api: [
    // Si le webhook doit Ãªtre adressÃ© Ã  plusieurs de vos API, dupliquer l'objet
    // ci-dessous.
    {
      // La clef API correspondante doit Ãªtre renseignÃ©e dans vos variables
      // d'environnement. A ce jour, PIERRE autorise maximum 3 webhooks :
      // WEBHOOK_KEY_1, WEBHOOK_KEY_2 et WEBHOOK_KEY_3
      key: 'WEBHOOK_KEY_1',

      // L'URL Ã  laquelle PIERRE envoie le webhook (ici un exemple)
      url: 'ex : https://api.ikos.com',

      // Le format des donnÃ©es envoyÃ©es par le webhook. Il peut notamment
      // inclure des `custom_data` permettant d'identifier le locataire (cf.
      // custom_data plus haut). Ne manipuler que l'objet retournÃ©.
      // - custom_data: celles passÃ©e via le paramÃ¨tre de requÃªte `data`
      // - content : la question de l'utilisateur ou la rÃ©ponse de PIERRE
      // - role : user (= utilisateur), assistant ou system (= PIERRE)
      format: ({ custom_data, content, role }) => ({
        client_id: custom_data[0],
        date: Date.now(),
        content,
        role
      })
    }
  ],

  // Le modÃ¨le de langage utilisÃ© par PIERRE pour gÃ©nÃ©rer la rÃ©ponse finale.
  // Il est impÃ©ratif que la clef d'API correspondantes soit renseignÃ©e dans
  // les variables d'environnement.
  //
  // Voir https://github.com/charnould/pierre?tab=readme-ov-file#modÃ¨les-de-langage-ou-llm
  // pour plus d'informations sur les modÃ¨les de langage utilisÃ©s par PIERRE et
  // leur fonction, et leurs disponibilitÃ©s.
  //
  // Consulter https://ai-sdk.dev/docs/foundations/providers-and-models pour choisir
  // le modÃ¨le de langage le plus pertinent.
  //
  //
  models: {
    // Le modÃ¨le utilisÃ© pour augmenter/enrichir les requÃªtes de l'utilisateur.
    // Nous recommandons `qwen/qwen3-32b` via le moteur d'infÃ©rence Groq (https://groq.com/).
    // avec les paramÃ©trages suivants. L'objectif ici est de disposer d'un modÃ¨le
    // efficient et rapide.
    augment_with: {
      model: groq('qwen/qwen3-32b'),
      providerOptions: {
        groq: {
          reasoningFormat: 'raw',
          reasoningEffort: 'none',
          serviceTier: 'auto'
        }
      }
    },
    // Le modÃ¨le utlisÃ© par le reranker qui s'assure que les Ã©lÃ©ments de
    // rÃ©ponses retournÃ©s par les bases de connaissances sont pertinents.
    // Nous recommandons `qwen/qwen3-32b` via le moteur d'infÃ©rence Groq (https://groq.com/).
    // avec les paramÃ©trages suivants. L'objectif ici est de disposer d'un modÃ¨le
    // efficient et rapide.
    rerank_with: {
      model: groq('qwen/qwen3-32b'),
      providerOptions: {
        groq: {
          reasoningFormat: 'raw',
          reasoningEffort: 'default',
          serviceTier: 'auto'
        }
      }
    },

    // Le modÃ¨le qui gÃ©nÃ¨re les rÃ©ponses en s'appuyant sur les Ã©lÃ©ments les plus
    // pertinents retournÃ©s par le reranker. Il est fortement recommandÃ© de dÃ©sactiver
    // le "reasoning" lors de la gÃ©nÃ©ration de la rÃ©ponse finale pour rÃ©duire la latence.
    answer_with: {
      model: openai('gpt-4.1-mini'),
      providerOptions: {
        openai: {
          reasoningEffort: 'none',
          reasoningSummary: null
        }
      }
    }
  },

  //
  // Si `false` :
  //  - PIERRE sera accessible Ã  100 % des visiteurs sur internet.
  //  - Il s'agit du paramÃ©trage Ã  renseigner pour un chatbot accessible aux locataires.
  //
  // Si `true` :
  //  - PIERRE ne sera accessible qu'aux utilisateurs dÃ»ment habilitÃ©s et connectÃ©s.
  //  - C'est le paramÃ©trage Ã  choisir pour restreindre l'usage, par exemple, aux collaborateurs.
  //
  // Pour vous connecter la premiÃ¨re fois, saisissez `admin@pierre-ia.org` et
  // la valeur de la variable d'environnement `AUTH_PASSWORD`, puis crÃ©er des
  // utilisateurs.
  protected: false,

  // Quelles connaissances peut utiliser PIERRE lorsqu'il gÃ©nÃ¨re ses rÃ©ponses ?
  knowledge: {
    // Astuce : Si vous renseignez `false` pour l'ensemble des connaissances
    // ci-dessus, PIERRE se comportera comme un simple wrapper autour d'un LLM,
    // sans base de connaissances. Les rÃ©ponses seront quasi instantanÃ©es, mais
    // le risque d'hallucinations important.

    // Il est fortement recommandÃ© de renseigner `location` selon le format ci-dessous.
    // Cette information est injectÃ©e dans les prompts afin que lâ€™IA sache dans quel
    // territoire se situe lâ€™utilisateur. Sans cette prÃ©cision, lâ€™IA ne peut pas
    // dÃ©duire la localisation concernÃ©e : par exemple, Ã  la question Â« Quelles
    // sont les associations dâ€™aide au logement dans la rÃ©gion ? Â», elle ne
    // saura pas sâ€™il sâ€™agit de la rÃ©gion PACA, de lâ€™Occitanie ou dâ€™une autre.
    // Indiquer `null` pour ne pas fournir cette information.
    location: null, // 'Gers (32) en rÃ©gion Occitanie'

    // `community` correspond aux connaissances en open data de PIERRE. Il
    // s'agit de connaissances gÃ©nÃ©rales sur les HLM. En principe, `community`
    // doit toujours Ãªtre `true` pour rÃ©pondre en qualitÃ© aux questions.
    community: true,

    // `proprietary` correspond aux connaissances propres Ã  un organisme HLM,
    // qu'il ne souhaite pas partager avec `community` et qu'il gÃ¨re en son
    // nom propre.
    proprietary: false
  },
  // Le message qui s'affiche par dÃ©faut dans l'interface de PIERRE.
  greeting: [
    'Bonjour ğŸ–ï¸,',
    'Je suis PIERRE, une intelligence artificielle open source, personnalisable et plurilingue au service du mouvement HLM, de ses candidats, locataires et collaborateurs.',
    'Ma mission : rÃ©pondre 24/7/365 Ã  toutes les questions de Â« premier niveau Â» des candidats et locataires ou celles (plus complexes) des Ã©quipes.',
    "PS. Je n'ai pas connaissance Ã  ce jour des spÃ©cificitÃ©s des bailleurs."
  ],

  // Les exemples qui s'affichent par dÃ©faut dans l'interface de PIERRE.
  examples: [
    'Comment dÃ©poser mon prÃ©avis de congÃ© pour mon logement ? Et avez-vous un modÃ¨le de courrier ?',
    "Y-a-t-il des associations d'entraide dans le cadre de violences conjugales dans le Vaucluse ?",
    'EnquÃªte SLS, kÃ©zako + suis-je concernÃ© ?',
    "Qu'est-ce que l'avance Loca-Pass et comment savoir si j'y suis Ã©ligible ?",
    'Je cherche un logement social dans le Cantal. Comment dÃ©poser un dossier et quel est le processus ?',
    'ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ø¨Ù„Ø¯ÙŠØ© Grand Dijon HabitatØŸ'
  ],

  // Une mention qui s'affiche Ã  la fin de chaque rÃ©ponse de l'IA. Pour ne pas
  // afficher de mention, indiquer `null`.
  disclaimer: 'Une IA peut se tromper. VÃ©rifiez les informations importantes.'
} as Config
